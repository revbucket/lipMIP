{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: LipMIP\n",
    "In this tutorial, we'll go over how to use LipMIP to exactly compute the Lipschitz constant of a ReLU network. LipMIP leverages Gurobi as a solver for mixed-integer programs, and you'll need to have gurobi installed (and gurobipy) for this section to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('..')\n",
    "import torch \n",
    "from pprint import pprint \n",
    "\n",
    "import utilities as utils \n",
    "from relu_nets import ReLUNet \n",
    "import neural_nets.data_loaders as data_loaders\n",
    "import neural_nets.train as train \n",
    "from hyperbox import Hyperbox \n",
    "import interval_analysis as ia \n",
    "from lipMIP import LipMIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Basic usage\n",
    "\n",
    "In its simplest form, LipMIP frames evaluating the local Lipschitz constant of a scalar-valued ReLU network as a mixed-integer program. Crucially, there are two important components here: \n",
    "* Locality: We only support bounded hyperboxes as the domains in which we search over\n",
    "* Scalar-valued: The ReLU network must output a scalar (we extend to multivariate networks later). For a typical neural net, $f$, which maps $\\mathbb{R}^d\\rightarrow \\mathbb{R}^C$, we maintain a `c_vector`, $c$, and compute the Lipschitz constant of $\\langle c, f\\rangle$. For binary classification, $c$ is typically $[1,-1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an example, we'll consider binary classification over R^2\n",
    "DIMENSION = 2\n",
    "\n",
    "# Define an input domain and c_vector \n",
    "simple_domain = Hyperbox.build_unit_hypercube(DIMENSION)\n",
    "simple_c_vector = torch.Tensor([1.0, -1.0])\n",
    "\n",
    "# Build a random network and a LipMIP instance \n",
    "network_simple = ReLUNet([2, 16, 16, 2])\n",
    "simple_prob = LipMIP(network_simple, simple_domain, simple_c_vector, verbose=True, num_threads=2)\n",
    "\n",
    "simple_result = simple_prob.compute_max_lipschitz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can examine the result of this with the LipResult class \n",
    "simple_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(simple_result.as_dict().keys())  # A lot more information under the hood \n",
    "print('\\n')\n",
    "\n",
    "print(simple_result.squire.model) # like the gurobi.Model object we solved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Stopping Early\n",
    "There are two main techniques for early stopping which provides a certifiable **upper bound** to the lipschitz constant of a neural network. We describe how to include these options into LipMIP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Timeouts: If we only want to run LipMIP for a fixed clock time, this can be done by\n",
    "          setting the `timeout` parameter (in seconds) upon initialization\n",
    "Integrality_Gap: For maximization problems, MIP solves a sequence of relaxed problems which provide upper bounds\n",
    "                 to the true optimum. Any feasible point provides a lower bound. Using this, Gurobi keeps track \n",
    "                 of an integrality gap (as an upper bound on percent error). i.e. if we want to be within 10% of \n",
    "                 the right answer, we can set `mip_gap` to 0.10\n",
    "        \n",
    "In either case of stopping early, the returned value is an UPPER BOUND to the true Lipschitz constant of a network.\n",
    "\"\"\"\n",
    "# More complicated network for which timeouts matter \n",
    "network_bigger = ReLUNet([10, 10, 10, 10, 10, 2])\n",
    "bigger_domain = Hyperbox.build_unit_hypercube(10)\n",
    "bigger_c_vector = torch.Tensor([1.0, -1.0])\n",
    "\n",
    "timeout_prob = LipMIP(network_bigger, bigger_domain, bigger_c_vector, verbose=True, timeout=10.0)\n",
    "timeout_result = timeout_prob.compute_max_lipschitz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeout_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_prob = LipMIP(network_bigger, bigger_domain, bigger_c_vector, verbose=True, mip_gap=2)\n",
    "gap_result = gap_prob.compute_max_lipschitz() # This should take less than a minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Vector-valued Networks\n",
    "As per section 7 and appendix E of our paper, we can extend LipMIP to evaluating the Lipschitz constant of vector-valued networks where the norms of interest are linear norms. That is, if $f:\\mathbb{R}^n\\rightarrow \\mathbb{R}^m$ is a ReLU network, over some domain $\\mathcal{X}$, we can compute \n",
    "$$ \\max_{x,y\\in\\mathcal{X}} \\frac{||f(x)-f(y)||_b}{||x-y||_a}$$\n",
    "for some linear norms $||\\cdot||_a, ||\\cdot||_b$ over $\\mathbb{R}^n, \\mathbb{R}^m$ respectively. \n",
    "\n",
    "We describe the various norms our codebase can handle this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we handle the scalar-valued case. \n",
    "# The examples above have used abs(...) for the numerator and the L_inf norm for the denominator\n",
    "# To consider the L_1 norm in the denominator, adjust the `primal_norm` parameter\n",
    "\n",
    "\n",
    "# L-infinity norm (setting we use by default)\n",
    "linf_problem = LipMIP(network_simple, simple_domain, simple_c_vector, primal_norm='linf', verbose=True) \n",
    "linf_problem.compute_max_lipschitz()\n",
    "\n",
    "# L-1 norm\n",
    "l1_problem = LipMIP(network_simple, simple_domain, simple_c_vector, primal_norm='l1', verbose=True) \n",
    "l1_problem.compute_max_lipschitz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we consider the vector valued cases:\n",
    "# Case where both ||.||_a and ||.||_b are the infinity norms\n",
    "\n",
    "inf_problem = LipMIP(network_simple, simple_domain, 'l1Ball1', verbose=True) # c_vector -> 'l1Ball1'\n",
    "inf_problem.compute_max_lipschitz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we consider the case for CrossLipschitz robustness \n",
    "# We can encode these with c_vectors in the set \n",
    "# {'crossLipschitz', 'targetCrossLipschitz', 'trueCrossLipschitz', 'trueTargetCrossLipschitz'}\n",
    "# where the parameters with the 'targeted' tag are for the targeted setting \n",
    "# and the 'true' tag is a slower but more precise cross-lipschitz norm \n",
    "cross_problem = LipMIP(network_simple, simple_domain, 'crossLipschitz', verbose=True) # c_vector -> 'l1Ball1'\n",
    "cross_problem.compute_max_lipschitz()       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
