{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Neural Nets and Datasets\n",
    "In this first tutorial, we'll cover the basics of training neural networks and loading/generating datasets. We've extended pytorch neural networks to have a bunch of handy tools. We'll need all these tools to evaluate Lipschitz constants. Since we frequently operate with neural networks trained on real or synthetic datasets, with or without regularization, we cover some tools help us with these tasks.\n",
    "\n",
    "This Jupyter notebook is intended to be hosted by a server running in the main `LipMIP/` folder (so the imports play nice).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import things\n",
    "import sys \n",
    "sys.path.append('..')\n",
    "import torch \n",
    "\n",
    "import utilities as utils\n",
    "from relu_nets import ReLUNet \n",
    "import neural_nets.data_loaders as data_loaders\n",
    "import neural_nets.train as train\n",
    "import neural_nets.adv_attacks as adv_attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Building a neural net\n",
    "We only consider neural networks composed of compositions of affine and ReLU layers. We develop a particular type of pytorch `nn.Module` to encapsulate these networks only. These are initialized randomly and defined a priori based only on the size of each layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_A = ReLUNet([4,8,16,2], bias=True) # defines a network R^4->R^2 with fully connected layers with biases\n",
    "x = torch.rand((10, 4)) # 10 example inputs to network_A\n",
    "y = network_A(x) # we directly feed inputs to network_A \n",
    "print(\"Input: \", x)\n",
    "print(\"Output:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also recover the inputs to each ReLU unit as such \n",
    "preacts = network_A(x, return_preacts=True)\n",
    "print(len(preacts)) # i'th element is the input to the i'th relu (starting from 0)\n",
    "assert torch.all(y == preacts[-1]) # final element of preacts is the output of network_A(x)\n",
    "print([_.shape for _ in preacts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Loading or generating a dataset \n",
    "Here we describe how to load the MNIST dataset, as well as the medley of synthetic datasets we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The standard MNIST dataset can be loaded as\n",
    "mnist_train = data_loaders.load_mnist_data('train', batch_size=16, shuffle=True) # Training data\n",
    "mnist_val = data_loaders.load_mnist_data('val', batch_size=16, shuffle=True) # Validation data \n",
    "\n",
    "# We can collect and display MNIST images as such\n",
    "mnist_batch = next(iter(mnist_val))[0] \n",
    "utils.display_images(mnist_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To select only a subset of the MNIST digits,\n",
    "mnist17_train = data_loaders.load_mnist_data('train', digits=[1,7], batch_size=16, shuffle=True) # Training data\n",
    "mnist17_val = data_loaders.load_mnist_data('val', digits=[1,7], batch_size=16, shuffle=True) # Validation data \n",
    "\n",
    "mnist17_batch = next(iter(mnist17_val))[0] \n",
    "utils.display_images(mnist17_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We define several synthetic datasets. Primarily, we use one we'll call a Random K-Cluster\n",
    "This defines a collection of points over [0,1]^d where each has a label 1...C \n",
    "Parameters are the :\n",
    "    - num_points: number of elements in the dataset (training AND validation)\n",
    "    - dimension: specifies the d, where the points reside in [0,1]^d\n",
    "    - num_classes: number of distinct labels \n",
    "    - radius: how far each point (in l2 norm) must be from the other points \n",
    "    - k: number of 'leaders' we select \n",
    "    \n",
    "The data generation works by randomly sampling num_points points from [0,1]^d,\n",
    "such that they are all sufficiently separated. Then we randomly select k points to be 'leaders', \n",
    "and uniformly randomly assign each 'leader' a label. Then we assign every other point the label \n",
    "of their closest 'leader'.\n",
    "'''\n",
    "\n",
    "# Parameters of datasets are controlled with the RandomKParameters object \n",
    "data_params = data_loaders.RandomKParameters(num_points=512, dimension=2, num_classes=2, k=10, radius=0.01)\n",
    "\n",
    "# RandomDataset objects represent actual instantiations of a random dataset defined by the params above\n",
    "random_dataset = data_loaders.RandomDataset(data_params, random_seed=1234)\n",
    "\n",
    "random_train, random_val = random_dataset.split_train_val(0.75) # split data into training(75%) and val(25%) sets\n",
    "\n",
    "# If 2-dimensional, we can visualize the dataset as such\n",
    "random_dataset.plot_2d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Training neural nets\n",
    "With neural nets and datasets defined, we can perform training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_B = ReLUNet([2, 16, 16, 16, 2]) # make a new net to classify the random_dataset defined above\n",
    "\n",
    "# Parameters regarding how training are performed are contained within the TrainParameters object \n",
    "# By default, we use CrossEntropyLoss and the Adam optimizer with lr=0.001, and test after every epoch\n",
    "\n",
    "vanilla_train_params = train.TrainParameters(random_train, random_val, 500, # train for 500 epochs\n",
    "                                             test_after_epoch=100) # test after every 100 epochs\n",
    "\n",
    "train.training_loop(network_B, vanilla_train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can visualize the decision boundaries learned for networks with 2d inputs\n",
    "network_B.display_decision_bounds((0.0, 1.0), (0.0, 1.0), 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we can overlay the dataset on top of this\n",
    "ax = network_B.display_decision_bounds((0.0, 1.0), (0.0, 1.0), 100)\n",
    "random_dataset.plot_2d(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Training With Regularization\n",
    "We can incorporate custom regularizers into our training loop defined above. As an example, we'll apply standard Tikhonov (l2) regularization to the training of an MNIST network. We will also apply FGSM regularization against an $\\ell_\\infty$-bounded adversary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with l2-regularization\n",
    "network_MNIST = ReLUNet([784, 32, 32, 10]) # simple MNIST network \n",
    "\n",
    "# Reload the MNIST datasets\n",
    "mnist_train = data_loaders.load_mnist_data('train', batch_size=128, shuffle=True) # Training data\n",
    "mnist_val = data_loaders.load_mnist_data('val', batch_size=128, shuffle=True) # Validation data \n",
    "\n",
    "# Build the components of the loss function\n",
    "cross_entropy_loss = train.XEntropyReg(scalar=1.0)\n",
    "l2_loss = train.LpWeightReg(lp='l2', scalar=0.01) \n",
    "\n",
    "# Build the loss function to use \n",
    "loss_functional = train.LossFunctional(regularizers=[cross_entropy_loss, l2_loss])\n",
    "loss_functional.attach_network(network_MNIST)\n",
    "\n",
    "# Train the network \n",
    "mnist_train_params = train.TrainParameters(mnist_train, mnist_val, 10, loss_functional=loss_functional)\n",
    "train.training_loop(network_MNIST, mnist_train_params)\n",
    "\n",
    "# This can be sped up with the 'use_cuda=True' kwarg in training_loop(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with FGSM Regularizers \n",
    "network_MNIST.re_init_weights() # reset the weights to random \n",
    "\n",
    "# Build the FGSM loss \n",
    "fgsm_loss = train.LossFunctional(regularizers=[train.FGSM(0.1)]) #FGSM with adversary with 0.1 L_inf bound\n",
    "fgsm_loss.attach_network(network_MNIST)\n",
    "\n",
    "# Train the network \n",
    "mnist_train_params = train.TrainParameters(mnist_train, mnist_val, 10, loss_functional=loss_functional)\n",
    "train.training_loop(network_MNIST, mnist_train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the adversarially trained network\n",
    "import neural_nets.adv_attacks as adv_attacks\n",
    "adversary = adv_attacks.build_attack_partial(adv_attacks.fgsm,linf_bound=0.1)\n",
    "adv_attacks.eval_dataset(network_MNIST, mnist_val, adversary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
